{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdbb2b9e0b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset ,DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.dataset import Subset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, num_layers, hidden_dim, num_heads, mlp_dim, dropout_rate=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        num_patches_height = (image_size - patch_size) // patch_size + 1\n",
    "        num_patches_width = (image_size - patch_size) // patch_size + 1\n",
    "        num_patches = num_patches_height * num_patches_width\n",
    "\n",
    "        patch_dim = 256  # Adjust patch dimension to match positional embedding dimension\n",
    "        # num_patches = (image_size // patch_size) ** 2\n",
    "        # patch_dim = patch_size ** 2  # Assuming input images are RGB\n",
    "        \n",
    "        # Patch embedding layer\n",
    "        self.patch_embedding = nn.Conv2d(in_channels=1, out_channels=patch_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, patch_dim))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=patch_dim, nhead=num_heads), num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        # self.cls_token = nn.Parameter(torch.randn(32, patch_dim, 1, 1))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,patch_dim))\n",
    "        self.mlp_head = nn.Linear(patch_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embedding(x)\n",
    "        batch_size = patches.size(0)\n",
    "        patch_size = patches.size(2)\n",
    "        \n",
    "        print(\"Shape of patches:\", patches.shape)\n",
    "        print(\"Shape of positional embedding:\", self.positional_embedding.shape)\n",
    "        # Calculate the number of patches\n",
    "        num_patches_height = patches.size(2)\n",
    "        num_patches_width = patches.size(3)\n",
    "        num_patches = num_patches_height * num_patches_width\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        patches = patches + self.positional_embedding[:, :num_patches_height, :num_patches_width]\n",
    "        \n",
    "        # # Add positional embeddings old\n",
    "        # patches = patches + self.positional_embedding[:, :patch_size, :]\n",
    "        \n",
    "        # # # Concatenate classification token to patches\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1, -1)\n",
    "        print(patches.shape)\n",
    "        print(cls_token.shape)\n",
    "        \n",
    "        patches = torch.cat([cls_token, patches], dim=1)\n",
    "        \n",
    "        # Reshape patches for transformer input\n",
    "        patches = patches.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_output = self.encoder(patches)\n",
    "        \n",
    "        # Extract classification token representation\n",
    "        cls_token = encoder_output[0]  # First token is the classification token\n",
    "        \n",
    "        # Classification head\n",
    "        cls_token = self.dropout(cls_token)\n",
    "        output = self.mlp_head(cls_token)\n",
    "        \n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlzheimerDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the image categories.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # List the categories\n",
    "        categories = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "        label_mapping = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "        for category in categories:\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            for img_name in os.listdir(category_path):\n",
    "                self.image_paths.append(os.path.join(category_path, img_name))\n",
    "                self.labels.append(label_mapping[category])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale if not already\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for data augmentation\n",
    "from torch.utils.data import WeightedRandomSampler, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = AlzheimerDataset(root_dir='./Data', transform=transform)\n",
    "\n",
    "# Calculate class weights for balancing\n",
    "class_sample_count = np.array([len(np.where(np.array(dataset.labels) == t)[0]) for t in np.unique(dataset.labels)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in dataset.labels])\n",
    "\n",
    "# Define the sizes for train, validation, and test sets\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Extract the indices of the train_dataset to get the corresponding weights\n",
    "train_indices = train_dataset.indices\n",
    "train_weights = samples_weight[train_indices]\n",
    "\n",
    "# Create the sampler for the training set\n",
    "train_sampler = WeightedRandomSampler(train_weights, len(train_weights))\n",
    "\n",
    "# Number of workers for data loading\n",
    "num_workers = 0  # Adjust based on your systemâ€™s capability\n",
    "\n",
    "# Create the DataLoaders with the sampler for the training set\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=train_sampler,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True  # Helps with faster data transfer to CUDA\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show data shapes in 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([32, 1, 248, 496])\n",
      "Batch labels shape: torch.Size([32])\n",
      "Image 1 shape: torch.Size([1, 248, 496]), Label: 3\n",
      "Image 2 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 3 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 4 shape: torch.Size([1, 248, 496]), Label: 3\n",
      "Image 5 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 6 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 7 shape: torch.Size([1, 248, 496]), Label: 0\n",
      "Image 8 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 9 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 10 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 11 shape: torch.Size([1, 248, 496]), Label: 3\n",
      "Image 12 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 13 shape: torch.Size([1, 248, 496]), Label: 0\n",
      "Image 14 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 15 shape: torch.Size([1, 248, 496]), Label: 0\n",
      "Image 16 shape: torch.Size([1, 248, 496]), Label: 3\n",
      "Image 17 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 18 shape: torch.Size([1, 248, 496]), Label: 0\n",
      "Image 19 shape: torch.Size([1, 248, 496]), Label: 0\n",
      "Image 20 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 21 shape: torch.Size([1, 248, 496]), Label: 3\n",
      "Image 22 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 23 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 24 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 25 shape: torch.Size([1, 248, 496]), Label: 3\n",
      "Image 26 shape: torch.Size([1, 248, 496]), Label: 2\n",
      "Image 27 shape: torch.Size([1, 248, 496]), Label: 0\n",
      "Image 28 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 29 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 30 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 31 shape: torch.Size([1, 248, 496]), Label: 1\n",
      "Image 32 shape: torch.Size([1, 248, 496]), Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Fetch one mini-batch from the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f'Batch images shape: {images.shape}')  # Should be [batch_size, 1, height, width]\n",
    "    print(f'Batch labels shape: {labels.shape}')  # Should be [batch_size]\n",
    "\n",
    "    # Now iterate through the mini-batch to check each image and label\n",
    "    for i in range(images.size(0)):  # Loop through the batch\n",
    "        image_shape = images[i].shape\n",
    "        label = labels[i]\n",
    "        print(f'Image {i+1} shape: {image_shape}, Label: {label}')\n",
    "\n",
    "    # Break after one batch to limit output\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of patches: torch.Size([32, 256, 15, 31])\n",
      "Shape of positional embedding: torch.Size([1, 197, 256])\n",
      "torch.Size([32, 256, 15, 31])\n",
      "torch.Size([32, 1, 1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 15 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[97], line 51\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(patches\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(cls_token\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 51\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Reshape patches for transformer input\u001b[39;00m\n\u001b[1;32m     54\u001b[0m patches \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (seq_len, batch_size, embed_dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 15 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define your Vision Transformer model\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "num_classes = 4\n",
    "num_layers = 6\n",
    "hidden_dim = 256\n",
    "num_heads = 8\n",
    "mlp_dim = 512\n",
    "dropout_rate = 0.1\n",
    "vit_model = VisionTransformer(image_size, patch_size, num_classes, num_layers, hidden_dim, num_heads, mlp_dim, dropout_rate)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vit_model.parameters(), lr=0.001)\n",
    "\n",
    "# Define directory for model checkpoints\n",
    "checkpoint_dir = 'ModelCheckpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    vit_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    \n",
    "    # Validation phase\n",
    "    vit_model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = vit_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "\n",
    "    # Print epoch training and validation losses\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'vit_model_epoch_{epoch+1}.pt')\n",
    "    torch.save(vit_model.state_dict(), checkpoint_path)\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
